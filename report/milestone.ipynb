{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Report\n",
    "### Magdalena Barros, Paula Cadena, Michael Rosenbaum | CAPP 30254: Machine Learning for Public Policy \n",
    "\n",
    "This report describes our current progress to complete the CAPP 30254 proxy means test project. In our first two weeks, we've conducted a series of data exploration exercises to better define our training data as well as determined what our approach will be to implement and compare a set of models.\n",
    "\n",
    "Our approach will be to use a set of .py functions to analyze the data and create 3 candidate datasets to each contribute to the cleaning process. Then, we will create a data pipeline to train and evaluate the efficacy of 4 models we have covered in class:\n",
    "- Regression forests of some variety (XGBoost, standard CART)\n",
    "- K-Nearest Neighbors\n",
    "- Penalized multinomial logistic regressions of some variety (Elastic net, LASSO, etc.)\n",
    "- A simple Neural Net\n",
    "\n",
    "We will evaluate these models based on the model evaluation criteria we will learn in the remaining weeks of the course, likely focusing on predictive accuracy, not computation time due to the relatively smaller size of the data.\n",
    "\n",
    "## 1 | Data Exploration\n",
    "\n",
    "Our data exploration has focued on understanding the underlying dataset. The data comes from a [2017 ILO survey administered](https://webapps.ilo.org/surveyLib/index.php/catalog/7230/related-materials) by the Intra-American Development Bank.\n",
    "\n",
    "It is drawn from a nationally representative household survey and includes a subset of household- and individual-level variables that are cleaned by the IDB.\n",
    "\n",
    "So far, our main focus has been data quality assurance and data management. We have:\n",
    "- Loaded the data into Python;\n",
    "- Removed extraneous variables created by the IDB such as squared age that are fully dependent on underlying data from respondents. \n",
    "- Reviewed the survey documentation to understand the data generation process.\n",
    "- Evaluated data quality on missingness, item nonresponse, and \n",
    "- Created a development and training data split so that our internal analysis of missing\n",
    "\n",
    "### Results\n",
    "\n",
    "Three variables have missing values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Internal functions\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclean\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data\n\u001b[0;32m      8\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "# External packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Internal functions\n",
    "from model.clean import load_data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_df = load_data(\"train.csv\")\n",
    "cr_df.isnull().sum()[cr_df.isnull().sum() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome Distribution\n",
    "\n",
    "TODO: Outcome skewed (look at smote or ADAsyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decisions\n",
    "\n",
    "- Retain `hhsize` and drop `tamhog`, `r413`, `tamviv` as `hhsize` is calculated by the survey software based on the household roster, whereas each other is not aligned with the counts of household compositions variables (age, sex, etc.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details\n",
    "\n",
    "This has lead us to a foundational dataset with only data from the underlying population (and not IDB analysts) remaining, only single indicator variables for discrete variables (i.e., only keeping male from male and female), as well as summary statistics to describe missingness and item response rates for single selection items that are encoded as a series of indicator variables.\n",
    "\n",
    "We will create three sets of training data to apply our models on. These training sets aim to compare different approaches to defining the taregt population:\n",
    "\n",
    "1. **Parsimonious household-level dataset**: The smallest and simplest dataset we can create of household-level data. This assumes that the survey data contains a lot of noisy variables and attempts to select candidate variables most strongly associated with underlying proxies for poverty status: liquid and illiquid wealth, income, income potential, and resilience.\n",
    "1. **Complicated household-level dataset**: Keeping the data at the household-level, we will generate features from the individual-level data and a set of more complex interactions between candidate features.\n",
    "1. **Respondent-level dataset**: We will make individual predictions of poverty status based on individuals in each household and then train weights for the predictions for a composite. This will take advantage of variance in responses among household members in the survey to take advantage of predictors of non-classical measurement error associated with respondent differences.\n",
    "\n",
    "## 2 | Feature Engineering\n",
    "\n",
    "We have started to develop features with a focus on three initial types of features:\n",
    "\n",
    "- Comparisons between individual-level responses and household-responses (e.g., minimum education or number of students behind grade-level)\n",
    "- Collapsing categories for categoricl variables with low response rates.\n",
    "- Interactions between plausibly-related variables (e.g., water AND electricity)\n",
    "\n",
    "## 3 | Future work\n",
    "\n",
    "We have developed developed to-do list for future work to ensure that we are on track for completing the fina work.\n",
    "\n",
    "- \\[May 3\\] Create candidate datasets\n",
    "- \\[May 10\\] Model selection\n",
    "- \\[May 17\\] Model validation\n",
    "- \\[May 19\\] Report completion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "costa_rica_proxy_means_test-oy7JmmkA-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
